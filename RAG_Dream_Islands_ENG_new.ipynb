{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUY5otW6195ma8iNP+oNos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daria-Mir/dream_islands_RAG/blob/main/RAG_Dream_Islands_ENG_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Install and activate all necessary libraries**"
      ],
      "metadata": {
        "id": "vJQRiZsP9laD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnHAa9jT9ktZ"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "id": "YuO5u3gp-E4c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #for numerical operations\n",
        "import os, os.path as osp #for loading knowledge base files\n",
        "import pymupdf as pf #for operations with pdf files\n",
        "\n",
        "#Libraries for data cleaning\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Libraries for using Hugging Face (chunking, embeddings, vector database, RAG pipeline)\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "#Libraries for tokenization and embeddings\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "#Libraries for the Generation part (Gemini)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "_E77Rqpz-NmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Get access to GitHub repository where all files for the company RAG knowledge base are saved**"
      ],
      "metadata": {
        "id": "KZ9MZCSDJDON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the specific branch (data) of your GitHub repository into Colab\n",
        "!git clone --branch data https://github.com/Daria-Mir/dream_islands_RAG"
      ],
      "metadata": {
        "id": "lc0psV9SIhyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files in the 'data' branch directory\n",
        "data_dir = '/content/dream_islands_RAG'\n",
        "os.listdir(data_dir)"
      ],
      "metadata": {
        "id": "g3vJ6rz7JK68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Check that files all formats (.txt, .csv, .pdf) are connected to Colab**"
      ],
      "metadata": {
        "id": "AqwoX77sJ12R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read a .txt file\n",
        "txt_file_path = os.path.join(data_dir, 'islands.txt')\n",
        "with open(txt_file_path, 'r') as file:\n",
        "    text_content = file.read()\n",
        "\n",
        "print(text_content[:500])  # Display the first 500 characters"
      ],
      "metadata": {
        "id": "ZlgfNDRVJ5xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read a .pdf file\n",
        "pdf_file_path = os.path.join(data_dir, 'dream_islands_intro.pdf')\n",
        "doc = pf.open(pdf_file_path)\n",
        "\n",
        "# Extract text from the first page\n",
        "pdf_text = doc[1].get_text()\n",
        "print(pdf_text[:1500])  # Display the first 500 characters of the text"
      ],
      "metadata": {
        "id": "uhGFckVHJ9tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Initialize necessary instruments**\n",
        "\n",
        "\n",
        "*   Text Splitter for chunking\n",
        "*   Tokenizer\n",
        "*   Embeddings with Hugging Face\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "clXXRMAALaqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "o3sgtdQ6Lx2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Creation of Knowledge base**"
      ],
      "metadata": {
        "id": "NfasXyGtL9JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge base paths\n",
        "data_dir = '/content/dream_islands_RAG'\n",
        "txt_files = ['di_welness_art.txt', 'islands.txt', 'top_20_reatreats.txt', 'di_fasting_art.txt', 'di_workshops_description.txt', 'schedule_tanya.txt']\n",
        "pdf_files = ['activities.pdf', 'dream_islands_intro.pdf', 'tanya.pdf']"
      ],
      "metadata": {
        "id": "Yt1GzSVrME4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty knowledge base\n",
        "knowledge_base = []"
      ],
      "metadata": {
        "id": "6hTI6k71MHfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Pre-processing of files for the knowledge base**\n",
        "\n",
        "1.   Cleaning\n",
        "2.   Chunking\n",
        "3.   Tokenization\n",
        "1.   Adding all files in different formats to the knowledge base\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3eTGV6HGMWxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean the text data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = text.strip()  # Strip leading/trailing spaces\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "h4yZqVTNMY6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to chunk and clean text\n",
        "def preprocess_and_chunk(doc_text):\n",
        "    cleaned_text = clean_text(doc_text)\n",
        "    chunks = text_splitter.split_text(cleaned_text)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "LJHwLMTqMjNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize the chunks\n",
        "def tokenize_and_chunk(doc):\n",
        "    cleaned_doc = clean_text(doc)  # Clean the text\n",
        "    chunks = chunk_text(cleaned_doc, max_tokens=512)  # Chunk into manageable sizes\n",
        "    tokenized_chunks = [[tokenizer.encode(chunk, add_special_tokens=False)] for chunk in chunks]  # Tokenize each chunk\n",
        "    return tokenized_chunks"
      ],
      "metadata": {
        "id": "7KdHChHdNMLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and process each txt file\n",
        "for file_name in txt_files:\n",
        "    with open(os.path.join(data_dir, file_name), 'r') as txt_file:\n",
        "        doc_text = txt_file.read()\n",
        "        chunks = preprocess_and_chunk(doc_text)\n",
        "        knowledge_base.extend([tokenizer.encode(chunk) for chunk in chunks])\n",
        "\n",
        "# Read and process each pdf file\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_doc = pf.open(os.path.join(data_dir, pdf_file))\n",
        "    pdf_text = \"\\n\".join([page.get_text() for page in pdf_doc])\n",
        "    chunks = preprocess_and_chunk(pdf_text)\n",
        "    knowledge_base.extend([tokenizer.encode(chunk) for chunk in chunks])"
      ],
      "metadata": {
        "id": "4HJ4Z5pQN9PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first few chunks of text\n",
        "print(knowledge_base[:5])  # Display the first 5 chunks"
      ],
      "metadata": {
        "id": "4dB5X8DzOVQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Embedding with Hugging Face**"
      ],
      "metadata": {
        "id": "UvQw6RzJP1Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate embeddings for the tokenized chunks\n",
        "texts = [tokenizer.decode(chunk) for chunk in knowledge_base] # Decode tokenized chunks to get the original text\n",
        "embeddings = huggingface_embeddings.embed_documents(texts) # Embed the texts using the HuggingFaceEmbeddings object\n",
        "text_embedding_pairs = list(zip(texts, embeddings)) # Create pairs of text and embeddings"
      ],
      "metadata": {
        "id": "IttXnG-vP-Gj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create FAISS index from the embeddings\n",
        "faiss_index = FAISS.from_embeddings(text_embedding_pairs, huggingface_embeddings)"
      ],
      "metadata": {
        "id": "z4U9B0bEQC6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Creating RAG Pipeline with Gemini Generation**"
      ],
      "metadata": {
        "id": "rVZ8oT4IQVah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Google AI Studio API\n",
        "api_key = \"GOOGLE_API_KEY_HERE\"\n",
        "if not api_key:\n",
        "    raise ValueError(\"Please set GOOGLE_API_KEY environment variable\")"
      ],
      "metadata": {
        "id": "UdntO3c8Qs5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self, api_key):\n",
        "        print(\"Initializing RAG system...\")\n",
        "        # Retrieval configuration\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.sentences = knowledge_base  # Assuming knowledge_base is a list of tokenized chunks (lists of integers)\n",
        "        self.embeddings = self.model.encode([tokenizer.decode(chunk) for chunk in self.sentences], convert_to_tensor=True)  # Decode tokenized chunks\n",
        "\n",
        "        # Gemini configuration\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        # Generation configurations\n",
        "        generation_config = {\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_output_tokens\": 2048,\n",
        "        }\n",
        "\n",
        "        # Safety configurations\n",
        "        safety_settings = [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        ]\n",
        "\n",
        "        # Gemini model initialization\n",
        "        self.llm = genai.GenerativeModel(\n",
        "            model_name=\"gemini-1.5-flash\",\n",
        "            generation_config=generation_config,\n",
        "            safety_settings=safety_settings\n",
        "        )\n",
        "\n",
        "        print(\"RAG system successfully initialized!\")\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        \"\"\"Retrieve the top k most relevant documents\"\"\"\n",
        "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
        "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_k_indices:\n",
        "            # Decode the list of token IDs representing the chunk\n",
        "            # text = tokenizer.decode(self.sentences[idx][0], skip_special_tokens=True) #Modified\n",
        "            text = tokenizer.decode(self.sentences[idx]) #Modified\n",
        "            results.append({\n",
        "                'text': text,  # Now 'text' is the actual text string\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def generate(self, query, retrieved_docs):\n",
        "        \"\"\"Generate a response using Gemini\"\"\"\n",
        "        # Build the prompt with the retrieved context\n",
        "        context = \"\\n\".join([doc['text'] for doc in retrieved_docs])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "USER QUERY:\n",
        "{query}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "Generate a response to the user's question using ONLY the information provided in the context above.\n",
        "If some information is not present in the context, do not invent it.\n",
        "Provide a clear and well-structured response.\n",
        "\"\"\"\n",
        "\n",
        "        # Generate the response\n",
        "        response = self.llm.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    def query(self, user_query, k=3):\n",
        "        \"\"\"Complete RAG process: retrieval + generation\"\"\"\n",
        "        print(\"1. Retrieving relevant documents...\")\n",
        "        retrieved_docs = self.retrieve(user_query, k)\n",
        "\n",
        "        print(\"2. Generating response...\")\n",
        "        response = self.generate(user_query, retrieved_docs)\n",
        "\n",
        "        return {\n",
        "            'response': response,\n",
        "            'retrieved_docs': retrieved_docs\n",
        "        }\n",
        "\n",
        "def print_full_results(results):\n",
        "    \"\"\"Print the full results of the RAG system\"\"\"\n",
        "    print(\"\\nRetrieved Documents:\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, doc in enumerate(results['retrieved_docs'], 1):\n",
        "        print(f\"{i}. [Score: {doc['similarity']:.3f}] {doc['text']}\")\n",
        "\n",
        "    print(\"\\nGenerated Response:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(results['response'])"
      ],
      "metadata": {
        "id": "f53hjLqTRB6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9: Use the RAG**"
      ],
      "metadata": {
        "id": "APC1hAQMRP-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the system\n",
        "rag = RAGSystem(api_key)"
      ],
      "metadata": {
        "id": "l97HYcwSRXFT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask questions interactively\n",
        "while True:\n",
        "    user_query = input(\"Please ask a question (or type 'exit' to stop): \")\n",
        "\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Exiting the system...\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    response = rag.query(user_query)\n",
        "    print_full_results(response)"
      ],
      "metadata": {
        "id": "tv17AapERZBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}